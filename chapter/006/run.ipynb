{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# まずシンプルなRAGを実装\n",
    "# ====================================================\n",
    "# document_loadersを使ってGitHubのリポジトリからドキュメントをロード\n",
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter,\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding --> Chromaにインデクシング\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "db = Chroma.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、アプリケーションのライフサイクルの各段階を簡素化することを目的としており、以下のような機能を提供しています。\n",
      "\n",
      "1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\n",
      "\n",
      "2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\n",
      "\n",
      "3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\n",
      "\n",
      "LangChainは、チャットモデルや埋め込みモデル、ベクトルストアなどの主要コンポーネントに対して標準化されたインターフェースを提供し、開発者が異なるプロバイダー間で簡単に切り替えられるようにします。また、複雑なアプリケーションのためのオーケストレーション機能を提供し、アプリケーションの可視性と評価を向上させるためのツールも備えています。\n"
     ]
    }
   ],
   "source": [
    "# シンプルなRAGを実装\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\n",
    "    以下の文脈だけを踏まえて質問に回答してください。\n",
    "    文脈：\"\"\"\n",
    "    {context}\n",
    "    \"\"\"\n",
    "\n",
    "    質問：\n",
    "    {question}\n",
    "    ''')\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "output = chain.invoke(\"LangChainの概要を教えてください。\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検索クエリの工夫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# HyDE(Hypothetical Document Embeddings)\n",
    "# 質問に回答する仮説的な回答を推論させ、その出力をドキュメントの検索に使用する\n",
    "# ====================================================\n",
    "hy_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    以下の質問に回答する一文を作成してください\n",
    "\n",
    "    質問：{question}\n",
    "    \"\"\")\n",
    "\n",
    "hy_chain = hy_prompt | model | StrOutputParser()\n",
    "\n",
    "hyde_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": hy_chain | retriever,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "output = hyde_rag_chain.invoke(\"LangChainの概要を教えてください。\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 複数の検索クエリの生成\n",
    "# ====================================================\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class QueryGenerationOutput(BaseModel):\n",
    "    queries: list[str] = Field(description=\"質問に回答するための複数の検索クエリ\")\n",
    "\n",
    "\n",
    "query_generation_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "    質問に対してベクターデータベースから関連文書を検索するために、\n",
    "    3つの異なる検索クエリを生成してください。\n",
    "    距離ベースの類似性検索の限界を克服するために、\n",
    "    ユーザーの質問に対して複数の視点を提供することが目標です。\n",
    "\n",
    "    質問：{question}\n",
    "    \"\"\")\n",
    "\n",
    "query_generation_chain = (\n",
    "    query_generation_prompt | model.with_structured_output(QueryGenerationOutput) | (lambda x: x.queries)\n",
    ")\n",
    "\n",
    "multi_query_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": query_generation_chain | retriever.map(),  # mapで引数と戻り値をlist化できる\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "output = multi_query_rag_chain.invoke(\"LangChainの概要を教えてください。\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検索後の工夫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG-Fusionのイメージ図\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:893/1*EA6LTHgoZzc9RloYlKRIKw.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# RAG-Fusion\n",
    "# 複数の検索クエリを生成し、それらの検索結果をRRFで並べる手法\n",
    "# RRF(Reciprocal Rank Fusion)\n",
    "# ====================================================\n",
    "from IPython.display import Image\n",
    "\n",
    "print(\"RAG-Fusionのイメージ図\")\n",
    "Image(url=\"https://miro.medium.com/v2/resize:fit:893/1*EA6LTHgoZzc9RloYlKRIKw.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、アプリケーションのライフサイクルの各段階を簡素化することを目的としています。具体的には、以下のような機能を提供しています：\\n\\n- **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n- **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n- **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、チャットモデルや埋め込みモデル、ベクトルストアなどの関連技術に対する標準インターフェースを実装しており、数百のプロバイダーと統合されています。これにより、開発者は異なるコンポーネントを簡単に組み合わせたり、プロバイダーを切り替えたりすることができます。'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RRFの実装\n",
    "from collections import defaultdict\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "    retriever_outputs: list[list[Document]],\n",
    "    k: int = 60,\n",
    ") -> list[Document]:\n",
    "    # Documentオブジェクトとスコアの対応を保持する辞書を準備\n",
    "    doc_score_mapping = defaultdict(float)\n",
    "\n",
    "    # 各Documentオブジェクトを保持する辞書\n",
    "    content_to_doc = {}\n",
    "\n",
    "    # 検索クエリ毎にループ\n",
    "    for docs in retriever_outputs:\n",
    "        # 検索結果のドキュメントごとにループ\n",
    "        for rank, doc in enumerate(docs, start=1):\n",
    "            content = doc.page_content\n",
    "            # Documentオブジェクトを保存\n",
    "            content_to_doc[content] = doc\n",
    "            # スコアを加算\n",
    "            doc_score_mapping[content] += 1 / (rank + k)\n",
    "\n",
    "    # スコアの大きい順にソート\n",
    "    ranked = sorted(doc_score_mapping.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Documentオブジェクトのリストを返す\n",
    "    return [content_to_doc[content] for content, _ in ranked]\n",
    "\n",
    "\n",
    "rag_fusion_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": query_generation_chain | retriever.map() | reciprocal_rank_fusion,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_fusion_chain.invoke(\"LangChainの概要を教えてください。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクルの各段階を簡素化します。具体的には、以下のような機能を提供しています。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n\\n2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、チャットモデルや埋め込みモデル、ベクトルストアなどの関連技術に対する標準インターフェースを実装しており、数百のプロバイダーと統合されています。また、複数のオープンソースライブラリで構成されており、特に`langchain-core`、`langchain`、`langchain-community`、`langgraph`などのパッケージがあります。\\n\\nさらに、LangChainは、開発者がアプリケーションを構築する際に直面するさまざまな課題に対処するための標準化されたコンポーネントインターフェース、オーケストレーション機能、観測性と評価のサポートを提供しています。'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# リランクモデル\n",
    "# ====================================================\n",
    "from typing import Any\n",
    "\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def rerank(inp: dict[str, Any], top_n: int = 3) -> list[Document]:\n",
    "    question = inp[\"question\"]\n",
    "    documents = inp[\"documents\"]\n",
    "\n",
    "    cohere_reranker = CohereRerank(\n",
    "        model=\"rerank-multilingual-v3.0\", top_n=top_n\n",
    "    )  # top_n:リランク結果の上位何件を返すか\n",
    "\n",
    "    return cohere_reranker.compress_documents(documents=documents, query=question)\n",
    "\n",
    "\n",
    "rerank_rag_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"documents\": retriever}\n",
    "    | RunnablePassthrough.assign(context=rerank)\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rerank_rag_chain.invoke(\"LangChainの概要を教えてください。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG-Fusion-->リランクする例\n",
    "\n",
    "# rag_fusion_rerank_chain = (\n",
    "#     {\n",
    "#         \"question\": RunnablePassthrough(),\n",
    "#         \"documents\": query_generation_chain | retriever.map() | reciprocal_rank_fusion,\n",
    "#     }\n",
    "#     | RunnablePassthrough.assign(context=rerank)\n",
    "#     | prompt\n",
    "#     | model\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "# rag_fusion_rerank_chain.invoke(\"LangChainの概要を教えてください。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 複数のRetrieverを使う工夫\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025年1月現在の日本の首相は石破茂です。'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# LLMによるルーティング\n",
    "# 質問内容に応じて適切なRetriever（web検索orドキュメント検索）を選択する\n",
    "# ====================================================\n",
    "from enum import Enum\n",
    "\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "# トレースがわかりやすくなるよう、run_nameを設定\n",
    "langchain_document_retriever = retriever.with_config({\"run_name\": \"langchain_document_retriever\"})\n",
    "web_retriever = TavilySearchAPIRetriever(k=3).with_config({\"run_name\": \"web_retriever\"})\n",
    "\n",
    "\n",
    "# Retrieverの選択を行うチェーンを作成\n",
    "class Route(str, Enum):\n",
    "    langchain_document = \"langchain_document\"\n",
    "    web = \"web\"\n",
    "\n",
    "\n",
    "class RouteOutput(BaseModel):\n",
    "    route: Route\n",
    "\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "    質問に回答するために適切なRetrieverを選択してください。\n",
    "    質問：{question}\n",
    "    \"\"\")\n",
    "\n",
    "route_chain = route_prompt | model.with_structured_output(RouteOutput) | (lambda x: x.route)\n",
    "\n",
    "\n",
    "# ルーティングの結果を踏まえて検索するrouted_retriever関数と、処理全体の流れのChainを実装\n",
    "def routed_retriever(inp: dict[str, Any]) -> list[Document]:\n",
    "    question = inp[\"question\"]\n",
    "    route = inp[\"route\"]\n",
    "\n",
    "    if route == Route.langchain_document:\n",
    "        return langchain_document_retriever.invoke(question)\n",
    "    elif route == Route.web:\n",
    "        return web_retriever.invoke(question)\n",
    "\n",
    "    raise ValueError(f\"Unknown retriever: {route}\")\n",
    "\n",
    "\n",
    "route_rag_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"route\": route_chain}\n",
    "    | RunnablePassthrough.assign(context=routed_retriever)\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ドキュメント検索を選択する例\n",
    "# route_rag_chain.invoke(\"LangChainの概要を教えてください。\")\n",
    "\n",
    "# Web検索を選択する例\n",
    "route_rag_chain.invoke(\"2025年1月現在の日本の首相を教えてください\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、アプリケーションのライフサイクルの各段階を簡素化することを目的としており、以下のような機能を提供しています。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n\\n2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、さまざまなモデルや関連コンポーネントに対して標準化されたインターフェースを提供し、開発者がプロバイダー間で簡単に切り替えたり、コンポーネントを組み合わせたりできるようにします。また、複雑なアプリケーションの構築を支援するためのオーケストレーション機能や、アプリケーションの可視性と評価を向上させるためのツールも提供しています。'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# ハイブリッド検索\n",
    "# 埋め込みベクトルでの類似度検索 x BM25を使った全文検索\n",
    "# ====================================================\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# ドキュメント検索\n",
    "chroma_retriever = retriever.with_config({\"run_name\": \"chroma_retriever\"})\n",
    "# 全文検索\n",
    "bm25_retriever = BM25Retriever.from_documents(documents).with_config({\"run_name\": \"bm25_retriever\"})\n",
    "\n",
    "# 両方の検索を実施するChain\n",
    "hybrid_retriever = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"chroma_documents\": chroma_retriever,\n",
    "            \"bm25_documents\": bm25_retriever,\n",
    "        }\n",
    "    )\n",
    "    | (lambda x: [x[\"chroma_documents\"], x[\"bm25_documents\"]])\n",
    "    | reciprocal_rank_fusion\n",
    ")\n",
    "\n",
    "hybrid_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": hybrid_retriever,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "hybrid_rag_chain.invoke(\"LangChainの概要を教えてください。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
